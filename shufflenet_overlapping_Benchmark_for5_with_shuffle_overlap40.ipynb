{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "aqZ7F-q5A6lc"
      },
      "outputs": [],
      "source": [
        "import torch as t\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import math\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eROuxkNu4TAb",
        "outputId": "e60bb8bd-3fd6-43b9-a388-43e20317dd5a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "Ie4co-P6Pdk7"
      },
      "outputs": [],
      "source": [
        "import zipfile\n",
        "with zipfile.ZipFile(\"/content/drive/MyDrive/DataSets/dog&cat/train.zip\", 'r') as zip_ref:\n",
        "    zip_ref.extractall(\"/content\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_Mx_q8L44SHd"
      },
      "source": [
        "### build network"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "3KKYDKMk4SHe"
      },
      "outputs": [],
      "source": [
        "overlap_size = 40"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "CJs2QCkA9umU"
      },
      "outputs": [],
      "source": [
        "def channel_shuffle(x, groups=2):\n",
        "  bat_size, channels, w, h = x.shape\n",
        "  group_c = channels // groups\n",
        "  x = x.view(bat_size, groups, group_c, w, h)\n",
        "  x = t.transpose(x, 1, 2).contiguous()\n",
        "  x = x.view(bat_size, -1, w, h)\n",
        "  return x\n",
        "\n",
        "# used in the block\n",
        "def conv_1x1_bn(in_c, out_c, stride=1):\n",
        "  return nn.Sequential(\n",
        "    nn.Conv2d(in_c, out_c, 1, stride, 0, bias=False),\n",
        "    nn.BatchNorm2d(out_c),\n",
        "    nn.ReLU(True)\n",
        "  )\n",
        "\n",
        "def conv_bn(in_c, out_c, stride=2):\n",
        "  return nn.Sequential(\n",
        "    nn.Conv2d(in_c, out_c, 3, stride, 1, bias=False),\n",
        "    nn.BatchNorm2d(out_c),\n",
        "    nn.ReLU(True)\n",
        "  )\n",
        "\n",
        "\n",
        "class ShuffleBlock(nn.Module):\n",
        "  def __init__(self, in_c, out_c, downsample=False):\n",
        "    super(ShuffleBlock, self).__init__()\n",
        "    self.downsample = downsample\n",
        "    half_c = out_c // 2\n",
        "    input_overlap = (overlap_size*2)\n",
        "    input_with_overlap = (half_c-overlap_size)\n",
        "\n",
        "    if downsample:\n",
        "      self.branch1 = nn.Sequential(\n",
        "          # 3*3 dw conv, stride = 2\n",
        "          nn.Conv2d(in_c, in_c, 3, 2, 1, groups=in_c, bias=False),\n",
        "          nn.BatchNorm2d(in_c),\n",
        "          # 1*1 pw conv\n",
        "          nn.Conv2d(in_c, half_c, 1, 1, 0, bias=False),\n",
        "          nn.BatchNorm2d(half_c),\n",
        "          nn.ReLU(True)\n",
        "      )\n",
        "\n",
        "      self.branch2 = nn.Sequential(\n",
        "          # 1*1 pw conv\n",
        "          nn.Conv2d(in_c, half_c, 1, 1, 0, bias=False),\n",
        "          nn.BatchNorm2d(half_c),\n",
        "          nn.ReLU(True),\n",
        "          # 3*3 dw conv, stride = 2\n",
        "          nn.Conv2d(half_c, half_c, 3, 2, 1, groups=half_c, bias=False),\n",
        "          nn.BatchNorm2d(half_c),\n",
        "          # 1*1 pw conv\n",
        "          nn.Conv2d(half_c, half_c, 1, 1, 0, bias=False),\n",
        "          nn.BatchNorm2d(half_c),\n",
        "          nn.ReLU(True)\n",
        "      )\n",
        "\n",
        "      self.branch_overlaping = nn.Sequential(\n",
        "          # 3*3 dw conv, stride = 2\n",
        "          nn.Conv2d(in_c, in_c, 3, 2, 1, groups=in_c, bias=False),\n",
        "          nn.BatchNorm2d(in_c),\n",
        "          # 1*1 pw conv\n",
        "          nn.Conv2d(in_c, half_c, 1, 1, 0, bias=False),\n",
        "          nn.BatchNorm2d(half_c),\n",
        "          nn.ReLU(True)\n",
        "      )\n",
        "\n",
        "    else:\n",
        "      # in_c = out_c\n",
        "      assert in_c == out_c\n",
        "\n",
        "      self.branch1 = nn.Sequential(\n",
        "           # 1*1 pw conv\n",
        "          nn.Conv2d(input_with_overlap, half_c, 1, 1, 0, bias=False),\n",
        "          nn.BatchNorm2d(half_c),\n",
        "          nn.ReLU(True),\n",
        "          # 3*3 dw conv, stride = 1\n",
        "          nn.Conv2d(half_c, half_c, 3, 1, 1, groups=half_c, bias=False),\n",
        "          nn.BatchNorm2d(half_c)\n",
        "      )\n",
        "\n",
        "      self.branch2 = nn.Sequential(\n",
        "          # 1*1 pw conv\n",
        "          nn.Conv2d(input_with_overlap, half_c, 1, 1, 0, bias=False),\n",
        "          nn.BatchNorm2d(half_c),\n",
        "          nn.ReLU(True),\n",
        "          # 3*3 dw conv, stride = 1\n",
        "          nn.Conv2d(half_c, half_c, 3, 1, 1, groups=half_c, bias=False),\n",
        "          nn.BatchNorm2d(half_c),\n",
        "          # 1*1 pw conv\n",
        "          nn.Conv2d(half_c, half_c, 1, 1, 0, bias=False),\n",
        "          nn.BatchNorm2d(half_c),\n",
        "          nn.ReLU(True)\n",
        "      )\n",
        "\n",
        "      self.branch_overlaping = nn.Sequential(\n",
        "          # 1*1 pw conv\n",
        "          nn.Conv2d(input_overlap, half_c, 1, 1, 0, bias=False),\n",
        "          nn.BatchNorm2d(half_c),\n",
        "          nn.ReLU(True),\n",
        "          # 3*3 dw conv, stride = 1\n",
        "          nn.Conv2d(half_c, half_c, 3, 1, 1, groups=half_c, bias=False),\n",
        "          nn.BatchNorm2d(half_c),\n",
        "          # 1*1 pw conv\n",
        "          nn.Conv2d(half_c, half_c, 1, 1, 0, bias=False),\n",
        "          nn.BatchNorm2d(half_c),\n",
        "          nn.ReLU(True)\n",
        "      )\n",
        "\n",
        "\n",
        "  def forward(self, x):\n",
        "    out = None\n",
        "    if self.downsample:\n",
        "      # if it is downsampling, we don't need to do channel split\n",
        "      branch1_with_overlap= t.add(self.branch1(x) , self.branch_overlaping(x))\n",
        "      branch2_with_overlap= t.add(self.branch2(x) , self.branch_overlaping(x))\n",
        "      out = t.cat((branch1_with_overlap, branch2_with_overlap), 1)\n",
        "    else:\n",
        "      # channel split\n",
        "      channels = x.shape[1]\n",
        "      c = channels // 2\n",
        "      x1 = x[:, :(c-overlap_size), :, :]\n",
        "      x_overlap = x[:, (c-overlap_size):(c+overlap_size), :, :]\n",
        "      x2 = x[:, (c+overlap_size):, :, :]\n",
        "\n",
        "      branch1 = self.branch1(x1)\n",
        "      branch2 = self.branch2(x2)\n",
        "      branch_overlap = self.branch_overlaping(x_overlap)\n",
        "\n",
        "      branch1_with_overlap= t.add( branch1 , branch_overlap)\n",
        "      branch2_with_overlap= t.add(branch2 , branch_overlap)\n",
        "\n",
        "      out = t.cat((branch1_with_overlap, branch2_with_overlap), 1)\n",
        "    return channel_shuffle(out,2)\n",
        "\n",
        "\n",
        "class ShuffleNet2(nn.Module):\n",
        "  def __init__(self, num_classes=2, input_size=224, net_type=1):\n",
        "    super(ShuffleNet2, self).__init__()\n",
        "    assert input_size % 32 == 0 # 因为一共会下采样32倍\n",
        "\n",
        "\n",
        "    self.stage_repeat_num = [3,1,1]\n",
        "    if net_type == 0.5:\n",
        "      self.out_channels = [3, 24, 48, 96, 192, 1024]\n",
        "    elif net_type == 1:\n",
        "      self.out_channels = [3, 24, 116 , 116, 116 ,116]\n",
        "    elif net_type == 1.5:\n",
        "      self.out_channels = [3, 24, 176, 352, 704, 1024]\n",
        "    elif net_type == 2:\n",
        "      self.out_channels = [3, 24, 244, 488, 976, 2948]\n",
        "    else:\n",
        "      print(\"the type is error, you should choose 0.5, 1, 1.5 or 2\")\n",
        "\n",
        "    # let's start building layers\n",
        "    self.conv1 = nn.Conv2d(3, self.out_channels[1], 3, 2, 1)\n",
        "    self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
        "    in_c = self.out_channels[1]\n",
        "\n",
        "    self.stages = []\n",
        "    for stage_idx in range(len(self.stage_repeat_num)):\n",
        "      out_c = self.out_channels[2+stage_idx]\n",
        "      repeat_num = self.stage_repeat_num[stage_idx]\n",
        "      print(f\"in_c:{in_c} out_c:{out_c} repeat_num:{repeat_num}\")\n",
        "      for i in range(repeat_num):\n",
        "        if i == 0:\n",
        "          self.stages.append(ShuffleBlock(in_c, out_c, downsample=True))\n",
        "        else:\n",
        "          self.stages.append(ShuffleBlock(in_c, in_c, downsample=False))\n",
        "        in_c = out_c\n",
        "    self.stages = nn.Sequential(*self.stages)\n",
        "\n",
        "    in_c = self.out_channels[-2]\n",
        "    out_c = self.out_channels[-1]\n",
        "    self.conv5 = conv_1x1_bn(in_c, out_c, 1)\n",
        "    self.g_avg_pool = nn.AvgPool2d(kernel_size=(int)(input_size/32)) # 如果输入的是224，则此处为7\n",
        "\n",
        "    # fc layer\n",
        "    self.fc = nn.Linear(out_c, num_classes)\n",
        "\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = self.conv1(x)\n",
        "    x = self.maxpool(x)\n",
        "    x = self.stages(x)\n",
        "    x = self.conv5(x)\n",
        "    x = self.g_avg_pool(x)\n",
        "    x = x.view(-1, self.out_channels[-1])\n",
        "    x = self.fc(x)\n",
        "    return x\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torchvision import datasets, transforms\n",
        "import os\n",
        "import time\n",
        "from torch.utils import data\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "import copy"
      ],
      "metadata": {
        "id": "l638v8zsk4_j"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class DogCat(data.Dataset):\n",
        "  def __init__(self, root, trans=None, train=True, test=False):\n",
        "    self.test = test\n",
        "    self.train = train\n",
        "    imgs = [os.path.join(root, img) for img in os.listdir(root)]\n",
        "    '''\n",
        "    the format of test and trian image name is different\n",
        "    as for test: /test/102.jpg\n",
        "    as for train: /train/cat.1.jpg\n",
        "    '''\n",
        "    if test: # root: './dogvscat/test/' imgs = [\"xx/123.jpg\", \"xx/234.jpg\", ...]\n",
        "      sorted(imgs, key=lambda x: int(x.split(\".\")[-2].split(\"/\")[-1]))\n",
        "    else:\n",
        "      sorted(imgs, key=lambda x: int(x.split(\".\")[-2]))\n",
        "\n",
        "    # shuffle\n",
        "    np.random.seed(100)\n",
        "    imgs = np.random.permutation(imgs)\n",
        "\n",
        "    # split dataset\n",
        "    if self.test:\n",
        "      self.imgs = imgs\n",
        "    elif train:\n",
        "      self.imgs = imgs[:int(0.7*len(imgs))]\n",
        "    else:\n",
        "      self.imgs = imgs[int(0.7*len(imgs)):]\n",
        "\n",
        "    if trans==None:\n",
        "      normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                                        std=[0.229, 0.224, 0.225])\n",
        "      # test and dev dataset do not need to do data augemetation\n",
        "      if self.test or not self.train:\n",
        "        self.trans = transforms.Compose([\n",
        "                                        transforms.Resize(224),\n",
        "                                        transforms.CenterCrop(224),\n",
        "                                        transforms.ToTensor(),\n",
        "                                        normalize\n",
        "                                        ])\n",
        "      else:\n",
        "        self.trans = transforms.Compose([\n",
        "                                        transforms.Resize(256),\n",
        "                                        transforms.CenterCrop(224), # RandomSizedCrop(224)??\n",
        "                                        transforms.RandomHorizontalFlip(),\n",
        "                                        transforms.ToTensor(),\n",
        "                                        normalize\n",
        "                                        ])\n",
        "\n",
        "\n",
        "  def __getitem__(self, index):\n",
        "    '''\n",
        "    as for test: just return the id of picture.\n",
        "    as for train and dev: return 1 if dog, return 0 if cat\n",
        "    '''\n",
        "    imgpath = self.imgs[index]\n",
        "    if self.test:\n",
        "      label = int(imgpath.split(\".\")[-2].split(\"/\")[-1])\n",
        "    else:\n",
        "      kind = imgpath.split(\".\")[-3].split(\"/\")[-1]\n",
        "      label = 1 if kind == \"dog\" else 0\n",
        "    img = Image.open(imgpath)\n",
        "    img = self.trans(img)\n",
        "    return img, label\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.imgs)"
      ],
      "metadata": {
        "id": "PNIIE5O9k6db"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "val_result = []\n",
        "train_result = []"
      ],
      "metadata": {
        "id": "5c5pszZ0lHux"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(5):\n",
        "  print(f\"Period {i}\")\n",
        "  model = ShuffleNet2()\n",
        "  device = t.device(\"cuda\" if t.cuda.is_available() else \"cpu\")\n",
        "  model = model.to(device)\n",
        "\n",
        "\n",
        "  train_dataset = DogCat(\"/content/train\", train=True)\n",
        "  val_dataset = DogCat(\"/content/train\", train=False, test=False)\n",
        "  train_loader = data.DataLoader(train_dataset,\n",
        "                                 batch_size = 32,\n",
        "                                 shuffle=True\n",
        "                                 )\n",
        "  val_loader = data.DataLoader(val_dataset,\n",
        "                               batch_size = 32,\n",
        "                               shuffle=True)\n",
        "\n",
        "  dataloader = {}\n",
        "  dataloader[\"train\"] = train_loader\n",
        "  dataloader[\"val\"] = val_loader\n",
        "\n",
        "  device = t.device(\"cuda\" if t.cuda.is_available() else \"cpu\")\n",
        "  model = ShuffleNet2()\n",
        "  model = model.to(device)\n",
        "\n",
        "  loss_fn = nn.CrossEntropyLoss()\n",
        "  optimizer = t.optim.SGD(model.parameters(), lr=0.1, momentum=0.9)\n",
        "\n",
        "\n",
        "  def train_model(model, dataloaders, loss_fn, optimizer, num_epochs=5):\n",
        "    best_model_wts = copy.deepcopy(model.state_dict())\n",
        "    best_acc = 0.\n",
        "    val_loss_history = []\n",
        "    train_loss_history = []\n",
        "    for epoch in range(num_epochs):\n",
        "        for phase in [\"train\", \"val\"]:\n",
        "            running_loss = 0.\n",
        "            running_corrects = 0.\n",
        "            if phase == \"train\":\n",
        "                model.train()\n",
        "            else:\n",
        "                model.eval()\n",
        "\n",
        "            for inputs, labels in dataloaders[phase]:\n",
        "                inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "                with t.autograd.set_grad_enabled(phase==\"train\"):\n",
        "                    outputs = model(inputs) # bsize * 2 , because it is a binary classification\n",
        "                    loss = loss_fn(outputs, labels)\n",
        "\n",
        "                preds = outputs.argmax(dim=1)\n",
        "                if phase == \"train\":\n",
        "                    optimizer.zero_grad()\n",
        "                    loss.backward()\n",
        "                    optimizer.step()\n",
        "                running_loss += loss.item() * inputs.size(0)\n",
        "                running_corrects += t.sum(preds.view(-1) == labels.view(-1)).item()\n",
        "\n",
        "            epoch_loss = running_loss / len(dataloaders[phase].dataset)\n",
        "            epoch_acc = running_corrects / len(dataloaders[phase].dataset)\n",
        "\n",
        "            print(\"Phase {} loss: {}, acc: {}\".format(phase, epoch_loss, epoch_acc))\n",
        "\n",
        "            if phase == \"val\" and epoch_acc > best_acc:\n",
        "                best_acc = epoch_acc\n",
        "                best_model_wts = copy.deepcopy(model.state_dict())\n",
        "            if phase == \"val\":\n",
        "                val_loss_history.append(epoch_acc)\n",
        "            if(phase == \"train\"):\n",
        "                train_loss_history.append(epoch_acc)\n",
        "\n",
        "    model.load_state_dict(best_model_wts)\n",
        "    return model, val_loss_history , train_loss_history\n",
        "\n",
        "  model, val_logs , train_logs = train_model(model, dataloader, loss_fn, optimizer)\n",
        "  val_result.append(val_logs[-1])\n",
        "  train_result.append(train_logs[-1])\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iKyG1qp7lKOl",
        "outputId": "2bdd2204-baa5-4a38-ce52-43d8d1740cf8"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Period 0\n",
            "in_c:24 out_c:116 repeat_num:3\n",
            "in_c:116 out_c:116 repeat_num:1\n",
            "in_c:116 out_c:116 repeat_num:1\n",
            "in_c:24 out_c:116 repeat_num:3\n",
            "in_c:116 out_c:116 repeat_num:1\n",
            "in_c:116 out_c:116 repeat_num:1\n",
            "Phase train loss: 0.5974025117192949, acc: 0.6865714285714286\n",
            "Phase val loss: 0.5466202155590058, acc: 0.7373333333333333\n",
            "Phase train loss: 0.5036210659367698, acc: 0.7570857142857143\n",
            "Phase val loss: 0.6191339570363362, acc: 0.7034666666666667\n",
            "Phase train loss: 0.4257463686602456, acc: 0.8064571428571429\n",
            "Phase val loss: 0.4164143913427989, acc: 0.8136\n",
            "Phase train loss: 0.3797469530582428, acc: 0.8311428571428572\n",
            "Phase val loss: 0.3987717008610566, acc: 0.8252\n",
            "Phase train loss: 0.335908810206822, acc: 0.8537714285714286\n",
            "Phase val loss: 0.31631963580449424, acc: 0.8605333333333334\n",
            "Period 1\n",
            "in_c:24 out_c:116 repeat_num:3\n",
            "in_c:116 out_c:116 repeat_num:1\n",
            "in_c:116 out_c:116 repeat_num:1\n",
            "in_c:24 out_c:116 repeat_num:3\n",
            "in_c:116 out_c:116 repeat_num:1\n",
            "in_c:116 out_c:116 repeat_num:1\n",
            "Phase train loss: 0.6015144258362907, acc: 0.6792\n",
            "Phase val loss: 0.6622669075012207, acc: 0.6882666666666667\n",
            "Phase train loss: 0.4814236560549055, acc: 0.7704\n",
            "Phase val loss: 0.4635230514685313, acc: 0.7826666666666666\n",
            "Phase train loss: 0.42128232411657063, acc: 0.8107428571428571\n",
            "Phase val loss: 0.4088791223843892, acc: 0.8196\n",
            "Phase train loss: 0.3701056110211781, acc: 0.8372\n",
            "Phase val loss: 0.3702824735403061, acc: 0.8374666666666667\n",
            "Phase train loss: 0.31395239123276303, acc: 0.8657714285714285\n",
            "Phase val loss: 0.3967418253739675, acc: 0.8238666666666666\n",
            "Period 2\n",
            "in_c:24 out_c:116 repeat_num:3\n",
            "in_c:116 out_c:116 repeat_num:1\n",
            "in_c:116 out_c:116 repeat_num:1\n",
            "in_c:24 out_c:116 repeat_num:3\n",
            "in_c:116 out_c:116 repeat_num:1\n",
            "in_c:116 out_c:116 repeat_num:1\n",
            "Phase train loss: 0.5957783355099814, acc: 0.6841142857142857\n",
            "Phase val loss: 0.5542432522455851, acc: 0.7282666666666666\n",
            "Phase train loss: 0.4887593343121665, acc: 0.7654857142857143\n",
            "Phase val loss: 0.6439139657179515, acc: 0.7349333333333333\n",
            "Phase train loss: 0.4152724374089922, acc: 0.8133142857142858\n",
            "Phase val loss: 0.3877227792898814, acc: 0.824\n",
            "Phase train loss: 0.3746041361876896, acc: 0.8385714285714285\n",
            "Phase val loss: 0.45107166159947715, acc: 0.8133333333333334\n",
            "Phase train loss: 0.33167990687915255, acc: 0.852\n",
            "Phase val loss: 0.3641784295876821, acc: 0.8326666666666667\n",
            "Period 3\n",
            "in_c:24 out_c:116 repeat_num:3\n",
            "in_c:116 out_c:116 repeat_num:1\n",
            "in_c:116 out_c:116 repeat_num:1\n",
            "in_c:24 out_c:116 repeat_num:3\n",
            "in_c:116 out_c:116 repeat_num:1\n",
            "in_c:116 out_c:116 repeat_num:1\n",
            "Phase train loss: 0.5862752213954926, acc: 0.692\n",
            "Phase val loss: 0.5101550645192464, acc: 0.7493333333333333\n",
            "Phase train loss: 0.4660959547792162, acc: 0.7780571428571429\n",
            "Phase val loss: 0.5488679820696513, acc: 0.7481333333333333\n",
            "Phase train loss: 0.4109913893495287, acc: 0.8137714285714286\n",
            "Phase val loss: 0.4392653517405192, acc: 0.8034666666666667\n",
            "Phase train loss: 0.36547035433905467, acc: 0.8388\n",
            "Phase val loss: 0.3798538543701172, acc: 0.8352\n",
            "Phase train loss: 0.31799446174076623, acc: 0.8602857142857143\n",
            "Phase val loss: 0.3547839402914047, acc: 0.8518666666666667\n",
            "Period 4\n",
            "in_c:24 out_c:116 repeat_num:3\n",
            "in_c:116 out_c:116 repeat_num:1\n",
            "in_c:116 out_c:116 repeat_num:1\n",
            "in_c:24 out_c:116 repeat_num:3\n",
            "in_c:116 out_c:116 repeat_num:1\n",
            "in_c:116 out_c:116 repeat_num:1\n",
            "Phase train loss: 0.6017665336745126, acc: 0.6767428571428571\n",
            "Phase val loss: 0.5359509099324544, acc: 0.738\n",
            "Phase train loss: 0.49490072393417356, acc: 0.7654857142857143\n",
            "Phase val loss: 0.48495322012901304, acc: 0.7713333333333333\n",
            "Phase train loss: 0.41773500958851406, acc: 0.8096571428571429\n",
            "Phase val loss: 0.3939567501544952, acc: 0.8221333333333334\n",
            "Phase train loss: 0.359979416847229, acc: 0.8429142857142857\n",
            "Phase val loss: 0.5733573018391928, acc: 0.7981333333333334\n",
            "Phase train loss: 0.30858080465112414, acc: 0.8657714285714285\n",
            "Phase val loss: 0.3433123655676842, acc: 0.8525333333333334\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"training avg: { np.average(train_result)}\")\n",
        "print(f\"validation avg: { np.average(val_result)}\")\n",
        "\n",
        "print(f\"training max: { np.max(train_result)}\")\n",
        "print(f\"validation max: { np.max(val_result)}\")\n",
        "\n",
        "print(f\"training min: { np.min(train_result)}\")\n",
        "print(f\"validation min: { np.min(val_result)}\")"
      ],
      "metadata": {
        "id": "8E4YcnPFlNmB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1016b810-3506-4189-c9df-27b3434a6e93"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "training avg: 0.8595200000000001\n",
            "validation avg: 0.8442933333333332\n",
            "training max: 0.8657714285714285\n",
            "validation max: 0.8605333333333334\n",
            "training min: 0.852\n",
            "validation min: 0.8238666666666666\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "v64jXlOVnLM8"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}