{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "aqZ7F-q5A6lc"
      },
      "outputs": [],
      "source": [
        "import torch as t\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import math"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F18xOGKc3_Qv",
        "outputId": "7829a20e-ff87-4063-f9e4-c236e4d49a0a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "8aKZe2i8tTlA"
      },
      "outputs": [],
      "source": [
        "import zipfile\n",
        "with zipfile.ZipFile(\"/content/drive/MyDrive/DataSets/dog&cat/train.zip\", 'r') as zip_ref:\n",
        "    zip_ref.extractall(\"/content\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mb47FScT3u38"
      },
      "source": [
        "### build network"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "CJs2QCkA9umU"
      },
      "outputs": [],
      "source": [
        "def channel_shuffle(x, groups=2):\n",
        "  bat_size, channels, w, h = x.shape\n",
        "  group_c = channels // groups\n",
        "  x = x.view(bat_size, groups, group_c, w, h)\n",
        "  x = t.transpose(x, 1, 2).contiguous()\n",
        "  x = x.view(bat_size, -1, w, h)\n",
        "  return x\n",
        "\n",
        "# used in the block\n",
        "def conv_1x1_bn(in_c, out_c, stride=1):\n",
        "  return nn.Sequential(\n",
        "    nn.Conv2d(in_c, out_c, 1, stride, 0, bias=False),\n",
        "    nn.BatchNorm2d(out_c),\n",
        "    nn.ReLU(True)\n",
        "  )\n",
        "\n",
        "def conv_bn(in_c, out_c, stride=2):\n",
        "  return nn.Sequential(\n",
        "    nn.Conv2d(in_c, out_c, 3, stride, 1, bias=False),\n",
        "    nn.BatchNorm2d(out_c),\n",
        "    nn.ReLU(True)\n",
        "  )\n",
        "\n",
        "\n",
        "class ShuffleBlock(nn.Module):\n",
        "  def __init__(self, in_c, out_c, downsample=False):\n",
        "    super(ShuffleBlock, self).__init__()\n",
        "    self.downsample = downsample\n",
        "    half_c = out_c // 2\n",
        "    if downsample:\n",
        "      self.branch1 = nn.Sequential(\n",
        "          # 3*3 dw conv, stride = 2\n",
        "          nn.Conv2d(in_c, in_c, 3, 2, 1, groups=in_c, bias=False),\n",
        "          nn.BatchNorm2d(in_c),\n",
        "          # 1*1 pw conv\n",
        "          nn.Conv2d(in_c, half_c, 1, 1, 0, bias=False),\n",
        "          nn.BatchNorm2d(half_c),\n",
        "          nn.ReLU(True)\n",
        "      )\n",
        "\n",
        "      self.branch2 = nn.Sequential(\n",
        "          # 1*1 pw conv\n",
        "          nn.Conv2d(in_c, half_c, 1, 1, 0, bias=False),\n",
        "          nn.BatchNorm2d(half_c),\n",
        "          nn.ReLU(True),\n",
        "          # 3*3 dw conv, stride = 2\n",
        "          nn.Conv2d(half_c, half_c, 3, 2, 1, groups=half_c, bias=False),\n",
        "          nn.BatchNorm2d(half_c),\n",
        "          # 1*1 pw conv\n",
        "          nn.Conv2d(half_c, half_c, 1, 1, 0, bias=False),\n",
        "          nn.BatchNorm2d(half_c),\n",
        "          nn.ReLU(True)\n",
        "      )\n",
        "    else:\n",
        "      # in_c = out_c\n",
        "      assert in_c == out_c\n",
        "\n",
        "      self.branch2 = nn.Sequential(\n",
        "          # 1*1 pw conv\n",
        "          nn.Conv2d(half_c, half_c, 1, 1, 0, bias=False),\n",
        "          nn.BatchNorm2d(half_c),\n",
        "          nn.ReLU(True),\n",
        "          # 3*3 dw conv, stride = 1\n",
        "          nn.Conv2d(half_c, half_c, 3, 1, 1, groups=half_c, bias=False),\n",
        "          nn.BatchNorm2d(half_c),\n",
        "          # 1*1 pw conv\n",
        "          nn.Conv2d(half_c, half_c, 1, 1, 0, bias=False),\n",
        "          nn.BatchNorm2d(half_c),\n",
        "          nn.ReLU(True)\n",
        "      )\n",
        "\n",
        "\n",
        "  def forward(self, x):\n",
        "    out = None\n",
        "    if self.downsample:\n",
        "      # if it is downsampling, we don't need to do channel split\n",
        "      out = t.cat((self.branch1(x), self.branch2(x)), 1)\n",
        "    else:\n",
        "      # channel split\n",
        "      channels = x.shape[1]\n",
        "      c = channels // 2\n",
        "      x1 = x[:, :c, :, :]\n",
        "      x2 = x[:, c:, :, :]\n",
        "      out = t.cat((x1, self.branch2(x2)), 1)\n",
        "    return channel_shuffle(out, 2)\n",
        "\n",
        "\n",
        "class ShuffleNet2(nn.Module):\n",
        "  def __init__(self, num_classes=2, input_size=224, net_type=1):\n",
        "    super(ShuffleNet2, self).__init__()\n",
        "    assert input_size % 32 == 0 # 因为一共会下采样32倍\n",
        "\n",
        "\n",
        "    self.stage_repeat_num = [3, 1, 1]\n",
        "    if net_type == 0.5:\n",
        "      self.out_channels = [3, 24, 48, 96, 192, 1024]\n",
        "    elif net_type == 1:\n",
        "      self.out_channels = [3, 24, 116, 116, 116, 116]\n",
        "    elif net_type == 1.5:\n",
        "      self.out_channels = [3, 24, 176, 352, 704, 1024]\n",
        "    elif net_type == 2:\n",
        "      self.out_channels = [3, 24, 244, 488, 976, 2948]\n",
        "    else:\n",
        "      print(\"the type is error, you should choose 0.5, 1, 1.5 or 2\")\n",
        "\n",
        "    # let's start building layers\n",
        "    self.conv1 = nn.Conv2d(3, self.out_channels[1], 3, 2, 1)\n",
        "    self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
        "    in_c = self.out_channels[1]\n",
        "\n",
        "    self.stages = []\n",
        "    for stage_idx in range(len(self.stage_repeat_num)):\n",
        "      out_c = self.out_channels[2+stage_idx]\n",
        "      repeat_num = self.stage_repeat_num[stage_idx]\n",
        "      for i in range(repeat_num):\n",
        "        if i == 0:\n",
        "          self.stages.append(ShuffleBlock(in_c, out_c, downsample=True))\n",
        "        else:\n",
        "          self.stages.append(ShuffleBlock(in_c, in_c, downsample=False))\n",
        "        in_c = out_c\n",
        "    self.stages = nn.Sequential(*self.stages)\n",
        "\n",
        "    in_c = self.out_channels[-2]\n",
        "    out_c = self.out_channels[-1]\n",
        "    self.conv5 = conv_1x1_bn(in_c, out_c, 1)\n",
        "    self.g_avg_pool = nn.AvgPool2d(kernel_size=(int)(input_size/32)) # 如果输入的是224，则此处为7\n",
        "\n",
        "    # fc layer\n",
        "    self.fc = nn.Linear(out_c, num_classes)\n",
        "\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = self.conv1(x)\n",
        "    x = self.maxpool(x)\n",
        "    x = self.stages(x)\n",
        "    x = self.conv5(x)\n",
        "    x = self.g_avg_pool(x)\n",
        "    x = x.view(-1, self.out_channels[-1])\n",
        "    x = self.fc(x)\n",
        "    return x\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torchvision import datasets, transforms\n",
        "import os\n",
        "import time\n",
        "from torch.utils import data\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "import copy"
      ],
      "metadata": {
        "id": "AuWfurnqq6wi"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class DogCat(data.Dataset):\n",
        "  def __init__(self, root, trans=None, train=True, test=False):\n",
        "    self.test = test\n",
        "    self.train = train\n",
        "    imgs = [os.path.join(root, img) for img in os.listdir(root)]\n",
        "    '''\n",
        "    the format of test and trian image name is different\n",
        "    as for test: /test/102.jpg\n",
        "    as for train: /train/cat.1.jpg\n",
        "    '''\n",
        "    if test: # root: './dogvscat/test/' imgs = [\"xx/123.jpg\", \"xx/234.jpg\", ...]\n",
        "      sorted(imgs, key=lambda x: int(x.split(\".\")[-2].split(\"/\")[-1]))\n",
        "    else:\n",
        "      sorted(imgs, key=lambda x: int(x.split(\".\")[-2]))\n",
        "\n",
        "    # shuffle\n",
        "    np.random.seed(100)\n",
        "    imgs = np.random.permutation(imgs)\n",
        "\n",
        "    # split dataset\n",
        "    if self.test:\n",
        "      self.imgs = imgs\n",
        "    elif train:\n",
        "      self.imgs = imgs[:int(0.7*len(imgs))]\n",
        "    else:\n",
        "      self.imgs = imgs[int(0.7*len(imgs)):]\n",
        "\n",
        "    if trans==None:\n",
        "      normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                                        std=[0.229, 0.224, 0.225])\n",
        "      # test and dev dataset do not need to do data augemetation\n",
        "      if self.test or not self.train:\n",
        "        self.trans = transforms.Compose([\n",
        "                                        transforms.Resize(224),\n",
        "                                        transforms.CenterCrop(224),\n",
        "                                        transforms.ToTensor(),\n",
        "                                        normalize\n",
        "                                        ])\n",
        "      else:\n",
        "        self.trans = transforms.Compose([\n",
        "                                        transforms.Resize(256),\n",
        "                                        transforms.CenterCrop(224), # RandomSizedCrop(224)??\n",
        "                                        transforms.RandomHorizontalFlip(),\n",
        "                                        transforms.ToTensor(),\n",
        "                                        normalize\n",
        "                                        ])\n",
        "\n",
        "\n",
        "  def __getitem__(self, index):\n",
        "    '''\n",
        "    as for test: just return the id of picture.\n",
        "    as for train and dev: return 1 if dog, return 0 if cat\n",
        "    '''\n",
        "    imgpath = self.imgs[index]\n",
        "    if self.test:\n",
        "      label = int(imgpath.split(\".\")[-2].split(\"/\")[-1])\n",
        "    else:\n",
        "      kind = imgpath.split(\".\")[-3].split(\"/\")[-1]\n",
        "      label = 1 if kind == \"dog\" else 0\n",
        "    img = Image.open(imgpath)\n",
        "    img = self.trans(img)\n",
        "    return img, label\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.imgs)"
      ],
      "metadata": {
        "id": "OEwgRAe8q_an"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "val_result = []\n",
        "train_result = []"
      ],
      "metadata": {
        "id": "gJ73McIerZE7"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(5):\n",
        "  print(f\"Period {i}\")\n",
        "  model = ShuffleNet2()\n",
        "  device = t.device(\"cuda\" if t.cuda.is_available() else \"cpu\")\n",
        "  model = model.to(device)\n",
        "\n",
        "\n",
        "  train_dataset = DogCat(\"/content/train\", train=True)\n",
        "  val_dataset = DogCat(\"/content/train\", train=False, test=False)\n",
        "  train_loader = data.DataLoader(train_dataset,\n",
        "                                 batch_size = 32,\n",
        "                                 shuffle=True\n",
        "                                 )\n",
        "  val_loader = data.DataLoader(val_dataset,\n",
        "                               batch_size = 32,\n",
        "                               shuffle=True)\n",
        "\n",
        "  dataloader = {}\n",
        "  dataloader[\"train\"] = train_loader\n",
        "  dataloader[\"val\"] = val_loader\n",
        "\n",
        "  device = t.device(\"cuda\" if t.cuda.is_available() else \"cpu\")\n",
        "  model = ShuffleNet2()\n",
        "  model = model.to(device)\n",
        "\n",
        "  loss_fn = nn.CrossEntropyLoss()\n",
        "  optimizer = t.optim.SGD(model.parameters(), lr=0.1, momentum=0.9)\n",
        "\n",
        "\n",
        "  def train_model(model, dataloaders, loss_fn, optimizer, num_epochs=5):\n",
        "    best_model_wts = copy.deepcopy(model.state_dict())\n",
        "    best_acc = 0.\n",
        "    val_loss_history = []\n",
        "    train_loss_history = []\n",
        "    for epoch in range(num_epochs):\n",
        "        for phase in [\"train\", \"val\"]:\n",
        "            running_loss = 0.\n",
        "            running_corrects = 0.\n",
        "            if phase == \"train\":\n",
        "                model.train()\n",
        "            else:\n",
        "                model.eval()\n",
        "\n",
        "            for inputs, labels in dataloaders[phase]:\n",
        "                inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "                with t.autograd.set_grad_enabled(phase==\"train\"):\n",
        "                    outputs = model(inputs) # bsize * 2 , because it is a binary classification\n",
        "                    loss = loss_fn(outputs, labels)\n",
        "\n",
        "                preds = outputs.argmax(dim=1)\n",
        "                if phase == \"train\":\n",
        "                    optimizer.zero_grad()\n",
        "                    loss.backward()\n",
        "                    optimizer.step()\n",
        "                running_loss += loss.item() * inputs.size(0)\n",
        "                running_corrects += t.sum(preds.view(-1) == labels.view(-1)).item()\n",
        "\n",
        "            epoch_loss = running_loss / len(dataloaders[phase].dataset)\n",
        "            epoch_acc = running_corrects / len(dataloaders[phase].dataset)\n",
        "\n",
        "            print(\"Phase {} loss: {}, acc: {}\".format(phase, epoch_loss, epoch_acc))\n",
        "\n",
        "            if phase == \"val\" and epoch_acc > best_acc:\n",
        "                best_acc = epoch_acc\n",
        "                best_model_wts = copy.deepcopy(model.state_dict())\n",
        "            if phase == \"val\":\n",
        "                val_loss_history.append(epoch_acc)\n",
        "            if(phase == \"train\"):\n",
        "                train_loss_history.append(epoch_acc)\n",
        "\n",
        "    model.load_state_dict(best_model_wts)\n",
        "    return model, val_loss_history , train_loss_history\n",
        "\n",
        "  model, val_logs , train_logs = train_model(model, dataloader, loss_fn, optimizer)\n",
        "  val_result.append(val_logs[-1])\n",
        "  train_result.append(train_logs[-1])\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "62DJMeyrqseA",
        "outputId": "dfeab992-1fbf-4dc3-a33a-88c23f8a23df"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Period 0\n",
            "Phase train loss: 0.6101595587049212, acc: 0.6721142857142857\n",
            "Phase val loss: 0.554044464635849, acc: 0.7338666666666667\n",
            "Phase train loss: 0.48103493962969096, acc: 0.7685142857142857\n",
            "Phase val loss: 0.6758514631271362, acc: 0.6988\n",
            "Phase train loss: 0.4157906999179295, acc: 0.8112\n",
            "Phase val loss: 0.5803783712228139, acc: 0.7304\n",
            "Phase train loss: 0.3690317163331168, acc: 0.8338857142857143\n",
            "Phase val loss: 0.3760781307856242, acc: 0.8408\n",
            "Phase train loss: 0.3275513450690678, acc: 0.8603428571428572\n",
            "Phase val loss: 0.343012125770251, acc: 0.8396\n",
            "Period 1\n",
            "Phase train loss: 0.6171179124150957, acc: 0.6680571428571429\n",
            "Phase val loss: 0.5882863205591837, acc: 0.7014666666666667\n",
            "Phase train loss: 0.5078663537570408, acc: 0.7536571428571428\n",
            "Phase val loss: 0.513607220141093, acc: 0.7534666666666666\n",
            "Phase train loss: 0.44701926279067994, acc: 0.7907428571428572\n",
            "Phase val loss: 0.4321775094350179, acc: 0.8056\n",
            "Phase train loss: 0.3984975306034088, acc: 0.8232\n",
            "Phase val loss: 0.385369526052475, acc: 0.8304\n",
            "Phase train loss: 0.36007262073244367, acc: 0.8395428571428571\n",
            "Phase val loss: 0.4307976794163386, acc: 0.824\n",
            "Period 2\n",
            "Phase train loss: 0.6247801250594003, acc: 0.6565714285714286\n",
            "Phase val loss: 0.6597570591290792, acc: 0.6522666666666667\n",
            "Phase train loss: 0.5081189847809928, acc: 0.7554857142857143\n",
            "Phase val loss: 1.0167285478274029, acc: 0.6121333333333333\n",
            "Phase train loss: 0.4237807516949517, acc: 0.8082857142857143\n",
            "Phase val loss: 0.4947073866844177, acc: 0.7753333333333333\n",
            "Phase train loss: 0.36824558109555927, acc: 0.8378285714285715\n",
            "Phase val loss: 0.48255108048121137, acc: 0.7854666666666666\n",
            "Phase train loss: 0.3266633802652359, acc: 0.8602285714285715\n",
            "Phase val loss: 0.31386403633753457, acc: 0.8701333333333333\n",
            "Period 3\n",
            "Phase train loss: 0.6068749566214425, acc: 0.6705142857142857\n",
            "Phase val loss: 0.5665043813546499, acc: 0.7265333333333334\n",
            "Phase train loss: 0.513824416242327, acc: 0.7486285714285714\n",
            "Phase val loss: 0.50488048675855, acc: 0.7557333333333334\n",
            "Phase train loss: 0.455071971654892, acc: 0.7870285714285714\n",
            "Phase val loss: 0.610728349494934, acc: 0.7005333333333333\n",
            "Phase train loss: 0.40819484050273896, acc: 0.8143428571428571\n",
            "Phase val loss: 0.4190017380555471, acc: 0.8085333333333333\n",
            "Phase train loss: 0.3631581620863506, acc: 0.8397714285714286\n",
            "Phase val loss: 0.5312869074344635, acc: 0.7904\n",
            "Period 4\n",
            "Phase train loss: 0.6051620313780648, acc: 0.6761714285714285\n",
            "Phase val loss: 0.7528910187085469, acc: 0.6301333333333333\n",
            "Phase train loss: 0.5166461750234876, acc: 0.7520571428571429\n",
            "Phase val loss: 0.6770557276407878, acc: 0.6990666666666666\n",
            "Phase train loss: 0.4605435097694397, acc: 0.7862857142857143\n",
            "Phase val loss: 0.4506663828531901, acc: 0.7898666666666667\n",
            "Phase train loss: 0.39990387318474907, acc: 0.8205142857142858\n",
            "Phase val loss: 0.4591048660437266, acc: 0.8048\n",
            "Phase train loss: 0.3573646346296583, acc: 0.8443428571428572\n",
            "Phase val loss: 0.44643682130972545, acc: 0.8118666666666666\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"training avg: { np.average(train_result)}\")\n",
        "print(f\"validation avg: { np.average(val_result)}\")\n",
        "\n",
        "print(f\"training max: { np.max(train_result)}\")\n",
        "print(f\"validation max: { np.max(val_result)}\")\n",
        "\n",
        "print(f\"training min: { np.min(train_result)}\")\n",
        "print(f\"validation min: { np.min(val_result)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LzUl5v6ZaL9q",
        "outputId": "78da7a84-aa9e-44a6-ffea-c85f908af979"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "training avg: 0.8488457142857143\n",
            "validation avg: 0.8272\n",
            "training max: 0.8603428571428572\n",
            "validation max: 0.8701333333333333\n",
            "training min: 0.8395428571428571\n",
            "validation min: 0.7904\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "JJz-AGtPaNrS"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}